{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test RDD Examples\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello world!', 'this is an rdd assignment', 'good morning!!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1. Creating RDD's\n",
    "# 1. Using parallelize method\n",
    "rdd_par = spark.sparkContext.parallelize([\"hello world!\", \"this is an rdd assignment\", \"good morning!!\"])\n",
    "rdd_par.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an rdd assignment']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Using transformations\n",
    "rdd_trans = rdd_par.filter(lambda word:word.startswith('t'))\n",
    "rdd_trans.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Textbooks often simplify this to large-sample vs. small-sample methods use normal distribution with large samples and t-distribution with small samples. This is right almost all the time, because in real sampling problems we seldom have a basis for knowing σ. However, there can be some situations when we do have a basis for assuming a value for σ, such as using a σ based on past data, and in those situations even if sample size is small the correct procedure would be to use the normal distribution, so the simplified large-sample vs. small sample approach would lead to an error.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Loading the data from the datasource\n",
    "rdd_txt = spark.sparkContext.textFile(\"test.txt\")\n",
    "rdd_txt.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2. Read a text file and count the number of words in the file using RDD operations\n",
    "word_rdd = rdd_txt.flatMap(lambda word: word.split(' '))\n",
    "word_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 1),\n",
       " ('large-sample', 2),\n",
       " ('vs.', 2),\n",
       " ('use', 2),\n",
       " ('distribution', 1),\n",
       " ('large', 1),\n",
       " ('t-distribution', 1),\n",
       " ('is', 2),\n",
       " ('right', 1),\n",
       " ('in', 2),\n",
       " ('sampling', 1),\n",
       " ('we', 2),\n",
       " ('have', 2),\n",
       " ('basis', 2),\n",
       " ('σ.', 1),\n",
       " ('However,', 1),\n",
       " ('there', 1),\n",
       " ('situations', 2),\n",
       " ('when', 1),\n",
       " ('do', 1),\n",
       " ('value', 1),\n",
       " ('σ,', 1),\n",
       " ('as', 1),\n",
       " ('using', 1),\n",
       " ('based', 1),\n",
       " ('past', 1),\n",
       " ('even', 1),\n",
       " ('size', 1),\n",
       " ('correct', 1),\n",
       " ('procedure', 1),\n",
       " ('would', 2),\n",
       " ('simplified', 1),\n",
       " ('approach', 1),\n",
       " ('an', 1),\n",
       " ('Textbooks', 1),\n",
       " ('often', 1),\n",
       " ('simplify', 1),\n",
       " ('to', 3),\n",
       " ('small-sample', 1),\n",
       " ('methods', 1),\n",
       " ('normal', 2),\n",
       " ('with', 2),\n",
       " ('samples', 1),\n",
       " ('and', 2),\n",
       " ('small', 3),\n",
       " ('samples.', 1),\n",
       " ('This', 1),\n",
       " ('almost', 1),\n",
       " ('all', 1),\n",
       " ('the', 4),\n",
       " ('time,', 1),\n",
       " ('because', 1),\n",
       " ('real', 1),\n",
       " ('problems', 1),\n",
       " ('seldom', 1),\n",
       " ('a', 4),\n",
       " ('for', 3),\n",
       " ('knowing', 1),\n",
       " ('can', 1),\n",
       " ('be', 2),\n",
       " ('some', 1),\n",
       " ('assuming', 1),\n",
       " ('such', 1),\n",
       " ('σ', 1),\n",
       " ('on', 1),\n",
       " ('data,', 1),\n",
       " ('those', 1),\n",
       " ('if', 1),\n",
       " ('sample', 2),\n",
       " ('distribution,', 1),\n",
       " ('so', 1),\n",
       " ('lead', 1),\n",
       " ('error.', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3. Write a program to find the word frequency in a given file\n",
    "freq_words = word_rdd.map(lambda word: (word, 1))\n",
    "freq_words.reduceByKey(lambda a,b : a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TEXTBOOKS OFTEN SIMPLIFY THIS TO LARGE-SAMPLE VS. SMALL-SAMPLE METHODS USE NORMAL DISTRIBUTION WITH LARGE SAMPLES AND T-DISTRIBUTION WITH SMALL SAMPLES. THIS IS RIGHT ALMOST ALL THE TIME, BECAUSE IN REAL SAMPLING PROBLEMS WE SELDOM HAVE A BASIS FOR KNOWING Σ. HOWEVER, THERE CAN BE SOME SITUATIONS WHEN WE DO HAVE A BASIS FOR ASSUMING A VALUE FOR Σ, SUCH AS USING A Σ BASED ON PAST DATA, AND IN THOSE SITUATIONS EVEN IF SAMPLE SIZE IS SMALL THE CORRECT PROCEDURE WOULD BE TO USE THE NORMAL DISTRIBUTION, SO THE SIMPLIFIED LARGE-SAMPLE VS. SMALL SAMPLE APPROACH WOULD LEAD TO AN ERROR.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4. Write a program to convert all words in a file to uppercase\n",
    "rdd_txt.map(lambda word:word.upper()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['textbooks often simplify this to large-sample vs. small-sample methods use normal distribution with large samples and t-distribution with small samples. this is right almost all the time, because in real sampling problems we seldom have a basis for knowing σ. however, there can be some situations when we do have a basis for assuming a value for σ, such as using a σ based on past data, and in those situations even if sample size is small the correct procedure would be to use the normal distribution, so the simplified large-sample vs. small sample approach would lead to an error.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5. Write a program to convert all words in a file to lowercase\n",
    "rdd_txt.map(lambda word:word.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Textbooks',\n",
       " 'Often',\n",
       " 'Simplify',\n",
       " 'This',\n",
       " 'To',\n",
       " 'Large-sample',\n",
       " 'Vs.',\n",
       " 'Small-sample',\n",
       " 'Methods',\n",
       " 'Use',\n",
       " 'Normal',\n",
       " 'Distribution',\n",
       " 'With',\n",
       " 'Large',\n",
       " 'Samples',\n",
       " 'And',\n",
       " 'T-distribution',\n",
       " 'With',\n",
       " 'Small',\n",
       " 'Samples.',\n",
       " 'This',\n",
       " 'Is',\n",
       " 'Right',\n",
       " 'Almost',\n",
       " 'All',\n",
       " 'The',\n",
       " 'Time,',\n",
       " 'Because',\n",
       " 'In',\n",
       " 'Real',\n",
       " 'Sampling',\n",
       " 'Problems',\n",
       " 'We',\n",
       " 'Seldom',\n",
       " 'Have',\n",
       " 'A',\n",
       " 'Basis',\n",
       " 'For',\n",
       " 'Knowing',\n",
       " 'Σ.',\n",
       " 'However,',\n",
       " 'There',\n",
       " 'Can',\n",
       " 'Be',\n",
       " 'Some',\n",
       " 'Situations',\n",
       " 'When',\n",
       " 'We',\n",
       " 'Do',\n",
       " 'Have',\n",
       " 'A',\n",
       " 'Basis',\n",
       " 'For',\n",
       " 'Assuming',\n",
       " 'A',\n",
       " 'Value',\n",
       " 'For',\n",
       " 'Σ,',\n",
       " 'Such',\n",
       " 'As',\n",
       " 'Using',\n",
       " 'A',\n",
       " 'Σ',\n",
       " 'Based',\n",
       " 'On',\n",
       " 'Past',\n",
       " 'Data,',\n",
       " 'And',\n",
       " 'In',\n",
       " 'Those',\n",
       " 'Situations',\n",
       " 'Even',\n",
       " 'If',\n",
       " 'Sample',\n",
       " 'Size',\n",
       " 'Is',\n",
       " 'Small',\n",
       " 'The',\n",
       " 'Correct',\n",
       " 'Procedure',\n",
       " 'Would',\n",
       " 'Be',\n",
       " 'To',\n",
       " 'Use',\n",
       " 'The',\n",
       " 'Normal',\n",
       " 'Distribution,',\n",
       " 'So',\n",
       " 'The',\n",
       " 'Simplified',\n",
       " 'Large-sample',\n",
       " 'Vs.',\n",
       " 'Small',\n",
       " 'Sample',\n",
       " 'Approach',\n",
       " 'Would',\n",
       " 'Lead',\n",
       " 'To',\n",
       " 'An',\n",
       " 'Error.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6. Write a program to capitalize first letter of each words in file\n",
    "word_rdd.map(lambda cap:cap.capitalize()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t-distribution'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7. Find the longest length of word from given set of words\n",
    "word_rdd.map(lambda word:(len(word), word)).max()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ML', 1010), ('VLSI', 2002), ('ML', 1012), ('VLSI', 2020), ('ES', 3005), ('MSc', 4004), ('CC', 5002), ('BDA', 6011), ('HDA', 9002), ('BDA', 6022), ('MSc', 4023), ('CC', 5038), ('ES', 3033), ('BDA', 6045), ('CC', 5050), ('MSc', 4021), ('HDA', 9054), ('ML', 1055), ('VLSI', 2060), ('ES', 3045), ('HDA', 9080)]\n"
     ]
    }
   ],
   "source": [
    "# Q8. Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HDA, 1000 series ML, \n",
    "# 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. \n",
    "# Given registration number, generate a key-value pair of Registration Number and Corresponding Branch.\n",
    "reg_nos = [1010, 2002, 1012, 2020, 3005, 4004, 5002, 6011, 9002, 6022, 4023, 5038, \n",
    "           3033, 6045, 5050, 4021, 9054, 1055, 2060, 3045, 9080]\n",
    "reg_nos_rdd = spark.sparkContext.parallelize(reg_nos, 1)\n",
    "reg_nos_map = reg_nos_rdd.map(lambda reg:('ML', reg) if reg > 1000 and reg < 2000\n",
    "                             else ('VLSI', reg) if reg > 2000 and reg < 3000\n",
    "                             else ('ES', reg) if reg > 3000 and reg < 4000\n",
    "                             else ('MSc', reg) if reg > 4000 and reg < 5000\n",
    "                             else ('CC', reg) if reg > 5000 and reg < 6000\n",
    "                             else ('BDA', reg) if reg > 6000 and reg < 7000\n",
    "                             else ('HDA', reg))\n",
    "reg_nos_collect = reg_nos_map.collect()\n",
    "print(reg_nos_collect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. \n",
    "# One line may contain one or more numbers. Find the maximum, minimum, sum and mean of numbers.\n",
    "rdd_num = spark.sparkContext.textFile(\"numbers.txt\")\n",
    "numbers = rdd_num.flatMap(lambda no: no.split(\" \")).map(lambda num:int(num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the maximum number\n",
    "numbers.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the minimum number\n",
    "numbers.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2034"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the sum of numbers\n",
    "numbers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.43478260869566"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the mean of numbers\n",
    "numbers.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, \n",
    "# Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, \n",
    "# TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code.\n",
    "citizens_rdd = spark.sparkContext.textFile(\"citizen.txt\")\n",
    "states_rdd = spark.sparkContext.textFile(\"states.txt\")\n",
    "details = citizens_rdd.map(lambda word:word.split(\",\")).collect()\n",
    "state_codes = states_rdd.map(lambda state:state.split(\",\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Madhuri', '05-06-1997', '9876543210', 'madhuri@gmail.com', 'AP'],\n",
       " ['Mounika', '19-05-1998', '9780564326', 'mounika@gmail.com', 'TS'],\n",
       " ['Vaishnavi', '29-08-1998', '8750673528', 'vaishnavi@gmail.com', 'KL'],\n",
       " ['Renuka', '16-02-1997', '7690532360', 'renuka@gmail.com', 'GJ'],\n",
       " ['Meghana', '20-03-1997', '9086641207', 'meghana@gmail.com', 'NL'],\n",
       " ['Vasuki', '18-07-1997', '6306431870', 'vasuki@gmail.com', 'HR'],\n",
       " ['Priyanka', '04-09-1997', '90334681097', 'priyanka@gmail.com', 'MP'],\n",
       " ['Sneha', '03-12-1998', '9403450987', 'sneha@gmail.com', 'UP']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(details)):\n",
    "    for j in range(len(state_codes)):\n",
    "        if details[i][4] == state_codes[j][0]:\n",
    "            details[i][4] = state_codes[j][1]\n",
    "details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AndhraPradesh': 'AP',\n",
       " 'Telangana': 'TS',\n",
       " 'Kerala': 'KL',\n",
       " 'Gujarat': 'GJ',\n",
       " 'Nagaland': 'NL',\n",
       " 'Haryana': 'HR',\n",
       " 'MadhyaPradesh': 'MP',\n",
       " 'UttarPradesh': 'UP'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stateCode = states_rdd.map(lambda word:(word.split(\",\")[0],word.split(\",\")[1]))\n",
    "states_dict = {}\n",
    "for val in stateCode.collect():\n",
    "    states_dict[val[0]] = val[1]\n",
    "states_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Madhuri 05-06-1997 9876543210 madhuri@gmail.com AP',\n",
       " 'Mounika 19-05-1998 9780564326 mounika@gmail.com TS',\n",
       " 'Vaishnavi 29-08-1998 8750673528 vaishnavi@gmail.com KL',\n",
       " 'Renuka 16-02-1997 7690532360 renuka@gmail.com GJ',\n",
       " 'Meghana 20-03-1997 9086641207 meghana@gmail.com NL',\n",
       " 'Vasuki 18-07-1997 6306431870 vasuki@gmail.com HR',\n",
       " 'Priyanka 04-09-1997 90334681097 priyanka@gmail.com MP',\n",
       " 'Sneha 03-12-1998 9403450987 sneha@gmail.com UP']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = spark.sparkContext.broadcast(states_dict)\n",
    "def compress(state,codes):\n",
    "    dataSplit = state.split(\",\")\n",
    "    dataSplit[4] = codes.value.get(dataSplit[4])\n",
    "    data_new = ' '\n",
    "    data_new = data_new.join(dataSplit)\n",
    "    return data_new\n",
    "citizens_rdd.map(lambda word:compress(word, data)).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
